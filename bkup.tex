\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
\usepackage{mathrsfs,bbm,euscript,dsfont,amsmath,amsfonts,amssymb,amsthm}
\usepackage{wasysym,enumerate}
\def\rn{\mathbb{R}}
\def\cn{\mathbb{C}}
\def\nn{\mathbb{N}}
\def\dsig{{\mathcal{D}_\sigma}}
\def\dsign{{\mathcal{D}_\sigma^n}}
\def\ftiln{\widetilde{f}_\sigma^n}
\def\fsign{f_\sigma^n}
\def\l{\left}
\def\r{\right}
\def\fsigbar{\bar{f}_\sigma}
\def\ftar{f_{tar}}
\def\fobs{f_{obs}}
\def\fcon{f_{con}}
\def\dcon{\mathcal{D}_{con}}
\def\dtar{\mathcal{D}_{tar}}
\def\sF{\pazocal{F}}
\def\sG{\pazocal{G}}
\def\sM{\pazocal{M}}
\def\sX{\pazocal{X}}
\def\sD{\pazocal{D}}
\def\sB{\pazocal{B}}
\def\sV{\pazocal{V}}
\def\sH{\pazocal{H}}
\def\sP{\mathscr{P}}
\def\sQ{\mathscr{Q}}
\def\bX{\mathbf{X}}
\def\bt{\mathbf{t}}
\def\bc{\mathbf{c}}
\def\hs{\mathscr{HS}}
\def\pr{\mathbb{P}}
\def\bs{{\overset{\circ}{B}}}
\def\d2{\sD_2}
\def\dd{\Delta\left( \sD \right)}
\def\ind{\mathbbm{1}}
\def\span{\operatorname{span}}
\def\simiid{\overset{iid}{\sim}}
\newtheorem{lem}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{defin}{Definition}

%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{On The Identifiability of Mixture Models from Grouped Samples}


\author{
Robert A. Vandermeulen\\
Department of EECS\\
University of Michigan\\
Ann Arbor, MI 48109 \\
\texttt{rvdm@umich.edu} \\
\And
Clayton D. Scott \\
Deparment of EECS\\
Univeristy of Michigan\\
Ann Arbor, MI 48109 \\
\texttt{clayscot@umich.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
Finite mixture models are statistical models which appear in many problems in statistics and machine learning. In such models it is assumed that data are drawn from random probability measures, called mixture components, which are themselves drawn from a probability measure $\sP$  over probability measures. When estimating mixture models, it is common to make assumptions on the mixture components, such as parametric assumptions. In this paper, we make no assumption on the mixture components, and instead assume that observations from the mixture model are grouped, such that observations in the same group are known to be drawn from the same component. We show that any mixture of $m$ probability measures can be uniquely identified provided there are $2m-1$ observations per group. Moreover we show that, for any $m$, there exists a mixture of $m$ probability measures that cannot be uniquely identified when groups have $2m-2$ observations. Our results hold for any sample space with more than one element.
\end{abstract}
\section{Introduction}
A finite mixture model is a probability law based on a finite number of probability measures, $\mu_1, \ldots, \mu_m$, and a discrete distribution $w_1, \ldots, w_m$. A realization of a mixture model is generated by first generating a component at random $k$, $1 \le k \le m$, and then drawing from $\mu_k$. A mixture model can be associated with a probability measure on probability measures, which we denote $\sP$ with $\sP\left( \left\{ \mu_i \right\} \right)= w_i$ for all $i$. Mixture models are used to model data throughout statistics and machine learning.

A primary theoretical question concerning mixture models is identifiability. A mixture model is said to be identifiable if no other mixture model (of equal or lesser complexity) explains the distribution of the data. Some previous work on identifiability considers the situation where the observations are drawn iid from the mixture model, and conditions on $\mu_1, \ldots, \mu_m$ are imposed, such as Gaussianity \cite{dasgupta07,anderson14}. In this work we make no assumptions on $\mu_1,\cdots,\mu_m$. Instead, we assume the observations are grouped, such that realizations from the same group are known to be iid from the same component. We call these groups of samples ``random groups.'' We define a random group to be a random collection $\bX_i$, where $\bX_i =X_{i,1},\dots,X_{i,n} \simiid \nu_i$ and $\nu_i \simiid \sP$. We call the setting where $\sP$ is a probability measure over probability measures ($\sP$ is not necessarily atomic) and $\bX_i$ of the form described above, the {\em grouped sample setting}.

Consider the set of all mixtures of probability measures which yield the same distribution over the random groups as does $\sP$. If some element of this set other than $\sP$ has no more components than $\sP$ then $\sP$ is not identifiable. In other words, there is no way to differentiate $\sP$ from another model of equal or lesser complexity. Fortunately, with a sufficient number of samples in each random group, $\sP$ becomes the simplest model which describes the data. In this paper we show that, for any sample space, any mixture of probability measures with $m$ components is identifiable when there are $2m-1$ samples per random group. Furthermore we show that this bound cannot be improved, regardless of sample space. 
\subsection{Practical Implications of Results}
Though a somewhat mathematically abstract object, probability measures over spaces of probability measures arise quite naturally in many statistical problems. Any application which use mixture models, for example clustering, is utilizing a probability measure over probability measures. More recently there has been significant interest in the grouped sample setting. We will describe a few relevant machine learning problems here.

Transfer learning is the attempt to leverage several different but related training datasets to construct a classifier or regressor for another different but related testing dataset. In such a setting one may assume that there exists some probability measure over probability measures $\sP$ which is generating unobserved random measures $\l(\nu_1, \nu_2,\ldots\r)$. From these random measures we have access to $\l(\bX_1, \bX_2,\ldots\r)$. These groups of samples are our ``different but related training datasets'' with relatedness being induced through $\sP$. Finally one may assume that the testing dataset is generated from $\tilde{\nu}\sim \sP$ but with unobserved labels. This model has been used to construct effective transfer learning algorithms \cite{blanchard11,maurer13}.

Sometimes one would like to perform statistical techniques directly on a space of probability measures. Examples of this include detection of anomalous distributions \cite{muandet13} and distribution regression \cite{poczos13,szabo14}. Both of these problems assume a grouped sample setting.

In the grouped sample setting it is desirable to know how the number of samples in our random groups affects an algorithm's performance. For example: is having 10 samples per group enough to achieve satisfactory results for some application? In some settings group sample size may limited due to available data, but there are other considerations as well. For example in \cite{anandkumar14} large groups of samples are broken down to smaller groups of samples for algorithmic purposes. Furthermore, if we treat our random groups as one would typically treat a random vector, there may concerns that large random groups will cause issues because of the ``curse of dimensionality.'' If this is the case it might make sense to break down the random groups into smaller random groups. What is lost by throwing away such information? This question is rather deep and will likely require substantial investigation before being solved in a general sort of way. Our work stands as a concrete step towards resolving such questions by answering the question, ``if $\sP$ is an atomic probability measure over probability measures with $m$ atoms, what is the most samples per group we need to ensure that our data looks like it is generated uniquely from $\sP$ ?" We also introduce a collection of new mathematical techniques for approaching such problems.

\section{Related Work}
The question of how many samples are necessary in each random group to uniquely identify a finite mixture of probability measures has come up sporadically over the past couple of decades. The application of Kruskal's theorem \cite{kruskal77} has been used to concoct various identifiability results for random groups containing three samples. In \cite{allman09} it was shown that any mixture of linearly independent measures over a discrete space or linearly independent probability distributions on $\rn^d$ are identifiable from random groups containing three samples. The result most closely resembling our own is in \cite{rabani13}. This paper is primarily algorithmic; they introduce an algorithm for recovering $m$ mixture components with $2m-1$ samples per group in the discrete setting. They show that such an algorithm would not work for $2m-2$ samples per group. Unfortunately their proof techniques are fairly complicated and are inherently attached to the discrete setting. Our paper, on the other hand, is able to reach the same conclusion in a fully general setting using relatively simple arguments.
\section{Problem Setup} Proofs of all the lemmas in the remainder of the paper can be found in the supplemental material.
We will be treating this problem in as general of a setting as possible. For any measurable space we define $\delta_x$ as the Dirac measure at $x$. For $\smiley$ a set, $\sigma$-algebra, or measure, we denote $\smiley^{\times a}$ to be the standard $a$-fold product associated with that object. For any natural number $k$ we define $\left[ k \right] \triangleq \mathbb{N} \bigcap \left[ 1,k \right]$.
Let $\Omega$ be a set containing more than one element. This set is the sample space of our data. Let $\sF$ be a $\sigma$-algebra over $\Omega$. Assume $\sF \neq \left\{ \emptyset, \Omega \right\}$. We denote the space of probability measures over this space as $\sD\left( \Omega,\sF \right)$, which we will shorten to $\sD$. We will equip $\sD$ with the $\sigma$-algebra $2^\sD$ so that each Dirac measure over $\sD$ is unique. Define $\dd \triangleq \span\left( \delta_x: x \in \sD \right)$. This will be the ambient space where our mixtures of probability measures live. Let $\sP = \sum_{i=1}^m \delta_{\mu_i} w_i $ be a probability measure in $\dd$. Let $\mu\sim \sP$ and $X_1 ,\cdots, X_n \simiid \mu$. We will denote $\bX = \left( X_1,\cdots,X_n \right)$. $\bX$ is a random group from our mixture model.

We will now derive the probability law of $\bX$. Let $A\in \Omega^{\times n}$, we have
\begin{eqnarray*}
	\pr\left(\bX \in A \right)
	= \sum_{i=1}^m \pr\left( \bX \in A \right|\mu=\mu_i) \pr\left( \mu=\mu_i \right)
	=  \sum_{i=1}^m w_i \mu_i^{\times n}\left( A \right).
\end{eqnarray*}
The second equality follows from Lemma 3.10 in \cite{fomp}.
So the probability law of $\bX$ is 
\begin{eqnarray}
	\label{xdens}
	\sum_{i=1}^m w_i \mu_i^{\times n}. 
\end{eqnarray}
We want to view the probability law of $\bX$ as a function of $\sP$ in a mathematically rigorous way, which requires a bit of technical buildup.
Let $\sV$ be a vector space. We will now construct a version of the integral for $\sV$-valued functions over $\sD$. Let $\sQ\in \dd$. From the definition of $\dd$ it follows that $\sQ$ admits the representation $$\sQ = \sum_{i=1}^r \delta_{\mu_i} \alpha_i.$$
From the well-ordering principle there must exist some representation with minimal $r$ and we define such $r$ as the {\it order} of $\sQ$. 
\begin{defin} \label{def:mixmeasure}
	We call $\sP$ a {\em mixture of measures} if it is a probability measure in $\dd$. We will say that $\sP$ has $m$ {\em mixture components} if it has order $m$.
\end{defin}
We can show that the minimal representation of any $\sQ \in \dd$ is unique up to permutation of its indices.
\begin{lem} \label{lem:represent}
	Let $\sQ\in \dd$ admit minimal representations $\sQ = \sum_{i=1}^r \delta_{\mu_i} \alpha_i = \sum_{i=1}^r\delta_{\mu_i'} \alpha_i'$. There exists some permutation $\psi:\left[ r \right] \to \left[ r \right]$ such that $\mu_{\psi\left( i \right)} = \mu'_i$ and $\alpha_{\psi\left( i \right)} = \alpha'_i$ for all $i$.
\end{lem}
Henceforth when we define an element of $\dd$ with a summation we will assume that the summation is a minimal representation. Any minimal representation of a mixture of measures $\sP$ with $m$ components satisfies $\sP=\sum_{i=1}^m w_i \delta_{\mu_i}$ with $w_i>0$ for all $i$ and $\sum_{i=1}^m w_i = 1$. So any mixture of measures is a convex combination of Dirac measures at elements in $\sD$.

 For a function $f:\sD \to \sV$ define
\begin{eqnarray*}
	\int f(\mu) d\sQ(\mu) = \sum_{i=1}^r \alpha_i f\left( \mu_i \right),
\end{eqnarray*}
where $\sum_{i=1}^r \delta_{\mu_i} \alpha_i$ is a minimal representation of $\sQ$. This integral is well defined as a consequence of Lemma \ref{lem:represent}.

For a $\sigma$-algebra $\left(Q, \Sigma \right)$ we define $\sM \left(Q, \Sigma \right)$ as the space of all finite signed measures over that space. Let $\lambda_n:\sM\left( \Omega, \sF \right) \to \sM\left( \Omega^{\times n}, \sF^{\times n}\right);\mu \mapsto \mu^{\times n}$. We introduce the operator $V_n:\dd\to \sM\left( \Omega^{\times n}, \sF^{\times n} \right)$
\begin{eqnarray*}
	V_n(\sQ) = \int \lambda_n(\mu) d\sQ\left( \mu \right)= \int \mu^{\times n} d\sQ\left( \mu \right).
\end{eqnarray*}
For a minimal representation $\sQ =\sum_{i=1}^r \delta_{\mu_i} \alpha_i$, we have
\begin{eqnarray*}
	V_n(\sQ) =\sum_{i=1}^r \mu_i^{\times n} \alpha_i.
\end{eqnarray*}

From this definition we have that $V_n\left( \sP \right)$ is simply the law of $\bX$ which we derived earlier.

\begin{defin}\label{def:ident}
	We call a mixture of measures, $\sP$, \emph{$n$-identifiable} if there does not exist a different mixture of measures $\sP'$, with order no greater than the order of $\sP$, such that $V_n\left( \sP \right) = V_n\left( \sP' \right)$.
\end{defin}

Definition \ref{def:ident} is the central object of interest in this paper. Given a mixture of measures, $\sP = \sum_{i=1}^m w_i\delta_{\mu_i}$ then $V_n(\sP)$ is equal to $\sum_{i=1}^m w_i \mu_i^{\times n}$, the measure from which $\bX$ is drawn. In topic modelling $\bX$ would be the samples from a single document and in transfer learning it would  be one of the several collections of training samples. If $\sP$ is not $n$-identifiable then we know that there exists a mixture of measures which is no more complex (in terms of number of mixture components) than $\sP$ which is not discernible from $\sP$ given the data. Practically speaking this means we need more samples in each random group $\bX$ in order for the full richness of $\sP$ to be manifested in $\bX$.
\section{Results}
Our primary result gives us a bound on the $n$-identifiability of mixtures of measures with $m$ or fewer components. We also show that this bound is tight.
\begin{thm} \label{thm:ident}
	Let $\left( \Omega,\sF \right)$ be a measurable space. Mixtures of measures with $m$ components are $(2m-1)$-identifiable.
\end{thm}

\begin{thm} \label{thm:noident}
	Let $\left( \Omega, \sF \right)$ be a measurable space with $\sF \neq \left\{ \emptyset,\Omega \right\}$. For all $m$, there exists a mixture of measures with $m$ components which is not $(2m-2)$-identifiable.
\end{thm}
The following lemmas convey the unsurprising fact that identifiability is, in some sense, monotonic. 
\begin{lem}\label{lem:ident}
	If a mixture of measures is $n$-identifiable then it is $q$-identifiable for all $q>n$.
\end{lem}
\begin{lem} \label{lem:noident}
	If a mixture of measures is not $n$-identifiable then it is not $q$-identifiable for any $q<n$.
\end{lem}
Viewed alternatively these results say that $n=2m-1$ is the smallest value for which $V_{n}$ is injective over the set of all minimal mixtures of measures with $m$ or fewer components.
\section{Tensor Products of Hilbert Spaces}
Our proofs will rely heavily on the geometry of tensor products of Hilbert spaces which we will introduce in this section.

\subsection{Overview of Tensor Products}
First we introduce tensor products of Hilbert spaces. To our knowledge there does not exist a rigorous construction of the tensor product Hilbert space which is both succinct and intuitive. Because of this we will simply state some basic facts about tensor products of Hilbert spaces and hopefully instill some intuition for the uninitiated by way of example. A thorough treatment of tensor products of Hilbert spaces can be found in \cite{kadison83}.

Let $H$ and $H'$ be Hilbert spaces. From these two Hilbert spaces the ``simple tensors'' are elements of the form $h\otimes h'$ with $h\in H$ and $h' \in H'$. We can treat the simple tensors as being the basis for some inner product space $H_0$, with the inner product of simple tensors satisfying
\begin{eqnarray*}
	\l<h_1 \otimes h_1', h_2 \otimes h_2'\r> = \l<h_1,h_2\r>\l<h_1',h_2'\r>.
\end{eqnarray*}
The tensor product of $H$ and $H'$ is the completion of $H_0$ and is denoted $H\otimes H'$. To avoid potential confusion we note that notation just described is standard in operator theory literature. In some literature our definition of $H_0$ is denoted as $H\otimes H'$ and our definition of $H \otimes H'$ is denoted $H \widehat{\otimes} H'$.

As an illustrative example we consider the tensor product $L^2\left( \rn \right) \otimes L^2\left( \rn \right)$. It can be shown that there exists an isomorphism between $L^2\left( \rn \right) \otimes L^2\left( \rn \right)$ and $L^2(\rn^2)$ which maps the simple tensors to separable functions, $f \otimes f' \mapsto f(\cdot)f'(\cdot)$. We can demonstrate this isomorphism with a simple example. Let $f,g,f',g'\in L^2\left( \rn \right)$. Taking the $L^2(\rn^2)$ inner product of $f(\cdot)f'(\cdot)$ and $g(\cdot)g'(\cdot)$ gives us 
\begin{eqnarray*}
\int\int \l(f(x)f'(y)\r)\l(g(x)g'(y\r)) dx dy 
&=& \int f(x)g(x) dx \int f'(y)g'(y) dy\\
&=& \l<f,g\r>  \l<f',g'\r>\\
&=& \l<f\otimes f', g \otimes g'\r>.
\end{eqnarray*}
Beyond tensor product we will need to define tensor power. To begin we will first show that tensor products are, in some sense, associative. Let $H_1, H_2, H_3$ be Hilbert spaces. Proposition 2.6.5 in \cite{kadison83} states that there is a unique unitary operator, $U: (H_1 \otimes H_2)\otimes H_3 \to H_1 \otimes (H_2 \otimes H_3)$, which satisfies the following for all $h_1 \in H_1, h_2 \in H_2, h_3 \in H_3$,
\begin{eqnarray*}
	U\left( \left( h_1 \otimes h_2 \right)\otimes h_3 \right) = h_1 \otimes \left( h_2 \otimes h_3 \right).
\end{eqnarray*}
This implies that for any collection of Hilbert spaces, $H_1,\cdots , H_n$, the Hilbert space $H_1 \otimes \cdots \otimes H_n$ is defined unambiguously regardless of how we decide to associate the products. In the space $H_1 \otimes \cdots \otimes H_n$ we define a simple tensor as a vector of the form $h_1 \otimes\cdots\otimes h_n$ with $h_i \in H_i$. In \cite{kadison83} it is shown that $H_1 \otimes\cdots \otimes H_n$ is the closure of the span of these simple tensors. To conclude this primer on tensor products we introduce the following notation. For a Hilbert space $H$ we denote $H^{\otimes n}= \underbrace{H\otimes H \otimes \dots \otimes H}_\text{n times}$ and for $h \in H$, $h^{\otimes n}= \underbrace{h\otimes h \otimes \dots \otimes h}_\text{n times}$.

\subsection{Some Results for Tensor Product Spaces}
Here we will state technical results which will be useful for the rest of the paper. These lemmas are similar to or  are straightforward extensions of previous results which we needed to modify for our particular purposes. Let $\left( \Psi, \sG, \mu \right)$ be a $\sigma$-finite measure space. We have the following lemma which connects the $L^2$ space of products of measures to the tensor products of the $L^2$ space for each measure. This following lemma allows us to treat products of functions in $L^2$ as tensor products of functions. 
\begin{lem}
	\label{lem:l2prod}
	There exists a unitary transform $U:L^2\left( \Psi, \sG, \mu \right)^{\otimes n} \to L^2\left( \Psi^{\times n}, \sG^{\times n}, \mu^{\times n} \right)$ such that, for all $f_1,\cdots, f_n \in L^2\left( \Psi, \sG, \mu \right)$, $U\left( f_1\otimes \cdots \otimes f_n \right) = f_1(\cdot)\cdots f_n(\cdot)$.
\end{lem}
The following lemma used in the proof of Lemma \ref{lem:l2prod} as well as the proof of Theorem \ref{thm:noident}.
\begin{lem} \label{lem:unitprod}
	Let $H_1,\cdots, H_n, H_1',\cdots, H_n'$ be a collection of Hilbert spaces and $U_1,\cdots,U_n$ a collection of unitary operators with $U_i:H_i \to H_i'$ for all $i$. There exists a unitary operator $U:H_1 \otimes \cdots \otimes H_n \to H_1' \otimes \cdots \otimes H_n'$ satisfying $U\left( h_1 \otimes\cdots \otimes h_n \right) = U_1(h_1) \otimes \cdots \otimes U_n(h_n)$ for all $h_1 \in H_1 ,\cdots, h_n \in H_n$.
\end{lem}
A statement of the next lemma for $\rn^d$ can be found in \cite{symtensorrank}. We present our own proof for the Hilbert space setting in the supplemental material.
\begin{lem}\label{lem:linind}
	Let $n>1$ and let $h_1,\cdots, h_n$ be elements of a Hilbert space $H$ such that no elements are zero and no pairs of elements are collinear. Then $h_1^{\otimes n-1},\cdots h_n^{\otimes n-1}$ are linearly independent in $H^{\otimes n-1}$.
\end{lem}

\section{Proofs of Theorems}
With the tools developed in the previous sections we can now prove our theorems. First we introduce one additional piece of notation. For a function $p$ on a domain $\sX$ we define $p^{\times k}$ as simply the product of the function $k$ times on the domain $\sX^{\times k}$, $\underbrace{p(\cdot)\cdots p(\cdot)}_{\text{k times}}$. For a measure the notation continues to denote the standard product measure.

Finally will need the following technical lemma to connect the product of Radon-Nikodym derivatives to product measures.
\begin{lem} \label{lem:radprod}
	Let $\left( \Psi, \sG \right)$ be a measurable space, $\eta$ and $\gamma$ a pair of bounded measures on that space, and $f$ a nonnegative function in $L^1\left( \gamma \right)$ such that, for all $A \in \sG$, $\eta\left( A \right)=\int_A f d\gamma$. Then for all $n$, for all $B \in \sG^{\times n}$ we have
\begin{eqnarray*}
	\eta^{\times n}\left( B \right) = \int_B f^{\times n} d\gamma^{\times n}.
\end{eqnarray*}
\end{lem}
\begin{proof}[Proof of Theorem \ref{thm:ident}]
	We will proceed by contradiction. Suppose there exist two different mixtures of measures $\sP = \sum_{i=1}^l \delta_{\mu_i} a_i \neq \sP' = \sum_{j=1}^m \delta_{\nu_j}b_j$, such that
	\begin{eqnarray} \label{grr}
		\sum_{i=1}^{l} a_i {\mu_i}^{\times 2m-1} = \sum_{j=1}^{m} b_j {\nu}_j^{\times 2m-1}
	\end{eqnarray}
and $l\le m$. From our assumption on representation we know $\mu_i \neq \mu_j$ for all $i\neq j$ and similarly for $\nu_1,\cdots, \nu_m$. We will also assume that $\mu_i \neq \nu_j$ for all $i,j$. Were this not true we could simply subtract the smaller of the common terms from both sides of (\ref{grr}) and normalize to yield another pair of distinct mixtures of measures with fewer components and no shared terms, $\sQ, \sQ'$. Letting $\sQ$ have $m'$ components and $\sQ'$ have $l'$ with $m'\ge l'$ applying Lemma \ref{lem:noident} would give us $V_{2m'-1}\left( \sQ \right)= V_{2m'-1}\left( \sQ' \right)$ and we could proceed as usual.

	Let $\xi = \sum_{i=1}^l \mu_i + \sum_{j=1}^m \nu_j$. Clearly $\xi$ dominates $\mu_i$ and $\nu_j$ for all $i,j$ so we can define Radon-Nikodym derivatives $p_i = \frac{d \mu_i}{d \xi}$, $q_j = \frac{d \nu_j}{d \xi}$ which are in $L^1\left( \Omega , \sF, \xi \right)$. We can assert that these derivatives are everywhere nonnegative without issue. Clearly no two of these derivatives are equal. If one of the derivatives were a scalar multiple of another, for example $p_1 = \alpha p_2$ for some $\alpha \neq 1$, it would imply
\begin{eqnarray*}
	\mu_1\left( \Omega \right) = \int_{\Omega} p_1 d\xi = \int \alpha p_2 d\xi=\alpha.
\end{eqnarray*}
This is not true so no pair of these derivatives are collinear.

Lemma \ref{lem:radprod} tells us that, for any $R \in \sF^{\times 2m-1}$  we have
\begin{eqnarray*}
	\int_R \sum_{i=1}^{l} a_i p_i^{\times 2m-1} d\xi^{\times 2m-1} 
	&=&  \sum_{i=1}^{l} a_i \mu_i^{\times 2m-1}\left( R \right)\\
	&=&  \sum_{j=1}^{m} b_j \nu_j^{\times 2m-1}\left( R \right)\\
	&=& \int_R \sum_{j=1}^{m} b_j q_j^{\times 2m-1}d\xi^{\times 2m-1}.
\end{eqnarray*}
Therefore
\begin{eqnarray} \label{eqn:radonequal}
	\sum_{i=1}^{l} a_i p_i^{\times 2m-1} = \sum_{j=1}^{m} b_j q_j^{\times 2m-1}
\end{eqnarray}
$\xi^{\times 2m-1}$-almost everywhere (Proposition 2.23 in \cite{folland99}).
We will now show for all $i,j$ that $p_i \in L^2 \left( \Omega, \sF, \xi \right)$ and $q_j \in L^2\left( \Omega, \sF, \xi \right)$. We will argue this for $p_1$ which will clearly generalize to the other elements. First we will show that $p_1 \le 1$ $\xi$-almost everywhere. Suppose this were not true and that there exists $A \in \sF$ with $\xi\left( A \right)>0$ and $p_1\left( A \right)>1$. Now we would have 
\begin{eqnarray*}
	\mu_1\left( A \right)
	=\int_A p_1 d\xi
	> \int_A 1 d\xi
	= \xi\left( A \right)
	=\sum_{i=1}^l\mu_i\left( A \right) + \sum_{j=1}^m \nu_j\left( A \right)
	\ge \mu_1\left( A \right)
\end{eqnarray*}
a contradiction. Evaluating directly we get
\begin{eqnarray*}
	\int p_1(\omega)^2 d \xi\left( \omega \right)
	&\le& \int 1 d \xi\left( \omega \right)\\
	&=& \xi\left( \Omega \right)\\
	&=& l+m,
\end{eqnarray*}
so $p_1 \in L^2\left( \Omega, \sF, \xi \right)$.
Applying the $U^{-1}$ operator from Lemma \ref{lem:l2prod} to (\ref{eqn:radonequal}) yields
\begin{eqnarray*}
		\sum_{i=1}^{l} a_i p_1^{\otimes 2m-1} = \sum_{j=1}^{m} b_j q_j^{\otimes 2m-1}.
\end{eqnarray*}
Since $l+m \le2m$ Lemma \ref{lem:linind} states that $p_1^{\otimes 2m-1},\cdots,p_{l}^{\otimes 2m-1},q_1^{\otimes 2m-1},\cdots,q_{m}^{\otimes 2m-1}$ are all linearly independent and thus $a_i = 0$ and $b_j = 0$ for all $i,j$, a contradiction.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:noident}]
To prove this theorem we will construct a pair of different mixture of measures, $\sP \neq \sP'$ which both contain $m$ components and satisfy $V_{2m-2}\left( \sP \right) = V_{2m-2}\left( \sP' \right)$.

From our definition of $\left( \Omega, \sF \right)$ we know there exists $F\in \sF$ such that $F, F^C$ are nonempty. Let $f\in F$ and $f' \in F^C$. It follows that $\delta_{f} \neq \delta_{f'}$ are different probability measures on $\left( \Omega, \sF \right)$. Because $\delta_{f}$ and $\delta_{f'}$ are dominated by $\xi = \delta_{f} + \delta_{f'}$ we know that there exists a pair of measurable functions $p, p'$ such that, for all $A$, $\delta_{f}\left( A \right) = \int_A p d\xi$ and $\delta_{f'}\left( A \right) = \int_{A} p'd\xi$. We can assert that $p$ and $p'$ are nonnegative without issue.

 From the same argument we used in the proof of Theorem \ref{thm:ident} we know $p,p' \in L^2\left( \Omega, \sF, \xi \right)$. Let $H_2$ be the Hilbert space generated from the span of $p,p'$. Let $(\varepsilon_i)_{i=1}^{2m}$ be $2m$ distinct elements of $\left[ 0,1 \right]$ and let $\left( p_i \right)_{i=1}^{2m}$ be elements of $L^1(\Omega, \sF, \xi)$ with $p_i = \varepsilon_i p + \left( 1-\varepsilon_i \right)p'$. Clearly $p_i$ is a pdf over $\xi$ for all $i$ and there are no pairs in this collection which are collinear. Let $H_2$ be the Hilbert space generated from the span of $p$ and $p'$. Since $H_2$ is isomorphic to $\rn^2$ there exists a unitary operator $U:H_2 \to \rn^2$. From Lemma \ref{lem:unitprod} there exists a unitary operator $U_{2m-2}:H_2^{\otimes 2m-2} \to {\rn^2}^{\otimes 2m-2}$ with $U_{2m-2}\left( h_1 \otimes\cdots \otimes h_{2m-2} \right) = U(h_1) \otimes \cdots \otimes U(h_{2m-2})$. Because $U$ is unitary the set $U_{2m-2}\left( \span\left( \left\{ h^{\otimes 2m-2}: h \in H_2 \right\} \right) \right)$ maps exactly to the set $\span\left( x^{\otimes 2m-2}:x \in \rn^2 \right)$.  An order $r$ tensor, $A_{i_1,\ldots,i_r}$, is {\it symmetric} if $A_{\psi\left( i_1 \right),\ldots,\psi\left( i_r \right)} = A_{i_1,\ldots,i_r}$for any $i_1,\cdots, i_r$ and permutation $\psi$. A consequence of Lemma 4.2 in \cite{symtensorrank} is that $\span\left( \l\{x^{\otimes 2m-2}:x \in \rn^2\r\} \right)\subset S^{2m-2}(\cn^2)$ is exactly the space of all symmetric order $2m-2$ tensors over $\cn^2$.

 From Proposition 3.4 in \cite{symtensorrank} it follows that the dimension of $S^{ 2m-2 }\left( \cn^2 \right)$ is $\left( \begin{array}{c} 2+ 2m-2-1 \\ 2m-2 \end{array} \right) = 2m-1$. From this we get that $\dim\left( \span\left( \left\{ h^{\otimes 2m-2}: h \in H_2 \right\} \right)\right)\le2m-1$.

The bound on the dimension of $\span\left( \left\{ h^{\otimes 2m-2}: h \in H_2 \right\} \right)$ implies that $\left( p_i^{\otimes 2m-2} \right)_{i=1}^{2m}$ are linearly dependent. Conversely Lemma \ref{lem:linind} implies that removing a single vector from $\left( p_i^{\otimes 2m-2} \right)_{i=1}^{2m}$ yields a set of vectors which are linearly independent. It follows that there exists $\left( \alpha_i \right)_{i=1}^{2m}$ with $\alpha_i \neq 0$ for all $i$ and
\begin{eqnarray*}
	\sum_{i=1}^{2m}\alpha_i p_i^{\otimes 2m-2} = 0.
\end{eqnarray*}
Without loss of generality we will assume that $\alpha_i<0$ for $i\in \left[ k \right]$ with $k\le m$. From this we have 
\begin{eqnarray}\label{foo}
	\sum_{i=1}^{k}-\alpha_i p_i^{\otimes 2m-2}=\sum_{j=k+1}^{2m}\alpha_j p_j^{\otimes 2m-2}.
\end{eqnarray}
From Lemma \ref{lem:l2prod} we have
\begin{eqnarray*}
	\sum_{i=1}^{k}-\alpha_i p_i^{\times 2m-2}=\sum_{j=k+1}^{2m}\alpha_j p_j^{\times 2m-2}
\end{eqnarray*}
and thus
\begin{eqnarray*}
	\int \sum_{i=1}^{k}-\alpha_i p_i^{\times 2m-2} d\xi^{\times 2m-2 }&=&\int \sum_{j=k+1}^{2m}\alpha_j p_j^{\times 2m-2}d\xi^{\times 2m-2 }\\
	 \Rightarrow \sum_{i=1}^{k}-\alpha_i &=&\sum_{j=k+1}^{2m}\alpha_j.
\end{eqnarray*}
Let $r=\sum_{i=1}^{k}-\alpha_i$. We know $r >0$ so dividing both sides of (\ref{foo}) by $r$ gives us
\begin{eqnarray*}
	\sum_{i=1}^{k}-\frac{\alpha_i}{r} p_i^{\otimes 2m-2}=\sum_{j=k+1}^{2m}\frac{\alpha_j}{r} p_j^{\otimes 2m-2}
\end{eqnarray*}
and the left and the right side are convex combinations. Let $\left( \beta_i \right)_{i=1}^{2m}$ positive numbers with $\beta_i = \frac{-\alpha_i}{r}$ for $i \in \left\{ 1,\cdots,k \right\}$ and $\beta_j = \frac{\alpha_j}{r}$ for $j\in \left\{ k+1,\cdots,2m \right\}$. This gives us
\begin{eqnarray*}
	\sum_{i=1}^{k}\beta_i p_i^{\otimes 2m-2}=\sum_{j=k+1}^{2m}\beta_j p_j^{\otimes 2m-2}.
\end{eqnarray*}
 It follows that
\begin{eqnarray*}
	\sum_{i=1}^{k} \beta_i p_i^{\otimes m-1}\otimes p_i^{\otimes m-1}&=&\sum_{j=k+1}^{2m}\beta_j p_j^{\otimes m-1}\otimes p_i^{\otimes m-1}.
\end{eqnarray*}
We will now show that $k=m$. Suppose $k<m$. Then $p_1^{\otimes m-1}, \cdots , p_{k+1}^{\otimes m-1}$ are linearly independent. From this we know that there exists $z$ such that $z\perp p_i^{\otimes m-1}$ for $i\in [k]$ but $z$ is not orthogonal to $p_{k+1}^{\otimes m-1}$. Using this vector we have
\begin{eqnarray*}
	\l<\sum_{i=1}^{k}\beta_i p_i^{\otimes 2m-1},z\otimes z\r>
	=\sum_{i=1}^{k}\beta_i\l<z, p_i^{\otimes m-1}\r>\l<z, p_i^{\otimes m-1}\r>
	=0 
\end{eqnarray*}
but
\begin{eqnarray*}
	\l< \sum_{i=k+1}^{2m}\beta_i p_i^{\otimes m-1}\otimes p_i^{\otimes m-1}, z\otimes z\r>
	= \sum_{i=k+1}^{2m}\beta_i\l<  p_i^{\otimes m-1}, z\r>\l<  p_i^{\otimes m-1}, z\r>
	>0
\end{eqnarray*}
and thus $k=m$.
Now we have
\begin{eqnarray*}
	\sum_{i=1}^m \beta_i p_i^{\otimes 2m-2} = \sum_{j=m+1}^{2m}\beta_j p_j^{\otimes 2m-2}.
\end{eqnarray*}
Applying Lemma \ref{lem:l2prod} we get that
\begin{eqnarray*}
	\sum_{i=1}^m \beta_i p_i^{\times 2m-2} = \sum_{j=m+1}^{2m}\beta_j p_j^{\times 2m-2}.
\end{eqnarray*}
From Lemma \ref{lem:radprod} we have,
\begin{eqnarray*}
	 \sum_{i=1}^m  \beta_i \left( \varepsilon_i \delta_f + \left( 1-\varepsilon_i \right) \delta_{f'} \right)^{\times 2m-2}   &=& \sum_{j=m+1}^{2m} \beta_j \left( \varepsilon_j \delta_f + \left( 1-\varepsilon_j \right) \delta_{f'} \right)^{\times 2m-2}.
\end{eqnarray*}
Setting $\mu_i = \left( \varepsilon_i \delta_{f} + \left( 1-\varepsilon_i \right)\delta_{f'} \right)$ yields
\begin{eqnarray*}
	\sum_{i=1}^m  \beta_i \mu_i^{\times 2m-2}  &=& \sum_{j=m+1}^{2m} \beta_j \mu_j^{\times 2m-2}.
\end{eqnarray*}
Thus setting $\sP = \sum_{i=1}^m  \beta_i \delta_{\mu_i}$ and $\sP' = \sum_{j=m+1}^{2m} \beta_j \delta_{\mu_j}$ gives us $V_{2m-2}\left( \sP \right) = V_{2m-2}\left( \sP' \right)$ and $\sP \neq \sP'$ by construction. 
\end{proof}
\section{Potential Algorithm and Conclusion}
When considering the tensor product structure used in our proofs, it seems likely that, with $2m$ samples per group, one could use spectral methods to recover the mixture components from an empirical estimate of
\begin{eqnarray*}
	\sum_{i=1}^{2m} a_i p_i^{\otimes m} \otimes p_i^{\otimes m} \cong \sum_{i=1}^{2m} a_i p_i^{\otimes m}  \l<p_i^{\otimes m},\cdot\r>.
\end{eqnarray*}
One could represent the elements $p_i$ using a kernel density estimator or a reproducing kernel Hilbert space embedding of the data with universal kernels \cite{christmann10}. We have no further suggestions on how to construct such an algorithm, but such an approach seems promising.

In this paper we have proven a fundamental bound on the identifiability of mixture models in a nonparametric setting. Any mixture with $m$ components is identifiable with groups of samples containing $2m-1$ samples from the same latent probability measure. We show that this bound is tight by constructing a mixture of $m$ probability measures which is not identifiable with groups of samples containing $2m-2$. These results hold for any mixture over any domain with at least two elements.
\bibliographystyle{plain}
\bibliography{rvdm}
\end{document}
